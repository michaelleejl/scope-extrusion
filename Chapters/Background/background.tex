\chapter{Background}
The aim of this dissertation is to evaluate, and propose, policies for mediating the interaction between Macocaml-style \textbf{metaprogramming} and \textbf{effect handlers}, by addressing the issue of \textbf{scope extrusion}.

In this chapter, I provide technical overviews of each of the key concepts: metaprogramming (\Cref{section:metaprogramming-technical}), effect handlers (\Cref{section:effects-technical}), and scope extrusion (\Cref{section:scope-extrusion-technical}).

\section{Metaprogramming}\label{section:metaprogramming-technical}
What is MacoCaml-style metaprogramming? I will provide an answer in two steps. First, I motivate metaprogramming, by illustrating the challenge of writing code that is both fast and maintainable (\Cref{subsection:metaprogramming-motivation}). Second, I will consider the design space of metaprogramming (\Cref{subsection:metaprogramming-design}), highlighting decisions made by MacoCaml.

\subsection{Metaprogramming for Fast and Maintainable Code}\label{subsection:metaprogramming-motivation}

Metaprogramming helps programmers write fast and maintainable code. How does one write fast and maintainable code? A naÃ¯ve answer is ``by being a skilled programmer''\footnote{Though not the worst answer: ``use ChatGPT''}. Programmer skill is insufficient, because maintainability and efficiency are in constant tension. 

I illustrate this tension by considering a concrete problem. Consider computing the gradient of a differentiable function as part of backpropagation over a neural network. More precisely, assume $f$ of type \mintinline{ocaml}{differentiable}

\begin{ocaml}
type differentiable = Sin | Tanh | Sigmoid | `$\ldots$`
                    | Polynomial of float list
                    | Compose of differentiable * differentiable
\end{ocaml}
For example, the following expression represents $\sin\circ \tanh$. 
% or $\cos \circ \tanh(x) $ depending on if the argument is \mintinline{ocaml}{true} or \mintinline{ocaml}{false}. 
\begin{ocaml}
Compose(Tanh, Sin)
\end{ocaml}

We wish to write a function \mintinline{ocaml}{grad} such that $\text{\mintinline{ocaml}{grad}} \, f = f'$. For simplicity, assume the existence of a helper function, \mintinline{ocaml}{grad_of}, that returns the gradient of basic functions. For example, \mintinline{ocaml}{grad_of Sin 0.0} $=$ \mintinline{ocaml}{cos 0.0} $=$ \mintinline{ocaml}{1.0}. \mintinline{ocaml}{grad_main} (\Cref{listing:ocaml-grad-main}) illustrates one way to compute gradients maintainably: 

\begin{code}
\begin{ocamllst}
let rec grad_main f x = match f with
  | Sin
  | Tanh
  | Sigmoid
  | `$\ldots$`
  | Polynomial(_) -> grad_of f x 
  | Compose(f, g) -> (grad_main f x) * (grad_main g (app f x))
\end{ocamllst}
\captionof{listing}{A maintainable implementation of \mintinline{ocaml}{grad}}
\label{listing:ocaml-grad-main}
\end{code}

However, \mintinline{ocaml}{grad_main} may not be the most efficient implementation. Performing a \mintinline{ocaml}{match} on every recursive call might result in expensive branches. If \mintinline{ocaml}{x} is a vector, and the weights of a polynomial are vectors, then \mintinline{ocaml}{grad_main} could hide opportunities for cache prefetching.

If $f$ is known in advance, for example, $f =$ \text{\mintinline{ocaml}{Compose(Tanh, Sin)}}, we could implement a more efficient \mintinline{ocaml}{grad_fast} function (\Cref{listing:ocaml-grad-fast}), whose body is simply a hardcoded equation:

\begin{code}
\begin{ocamllst}
let grad_fast x = (cos x) /. (cosh (sin x) ** 2)
\end{ocamllst}
\captionof{listing}{A fast implementation of \mintinline{ocaml}{grad}, assuming $f =$ \text{\mintinline{ocaml}{Compose(Tanh, Sin)}}}
\label{listing:ocaml-grad-fast}
\end{code}    

Although \mintinline{ocaml}{grad_fast} only works for a single $f$, it has eliminated the branching overhead, and enabled opportunities for prefetching. It is thus likely to be faster.

The \mintinline{ocaml}{grad} example illustrates the trade-off between maintainability and efficiency.\ \mintinline{ocaml}{grad_main} is maintainable in part because it parameterises over $f$. More generally, abstraction centralises implementations, thus reducing maintenance costs. However, \mintinline{ocaml}{grad_fast} is more efficient because it is more specialised to a specific $f$ (\text{\mintinline{ocaml}{Compose(Tanh, Sin)}}). More generally, many compiler optimisations, like monomorphisation, eliminate abstraction, simplifying functions by applying known arguments in advance. 

The tension between maintainability (abstraction) and efficiency (specialisation) has also been observed in regex matching \citep{tratt-2008}, parsing \citep{yallop-2023}, linking \citep{servetto-2013}, statistical modelling \citep{wickham-2019}, and hardware design \citep{vandebon-2021}.

A more informed answer might therefore be ``by letting the compiler generate optimised versions of my maintainable code''. Not quite: for reasons both theoretical and practical, compiler optimisations can be insufficient. In theory, we proposed an optimisation that assumed we would always know $f$ at, or before, compile-time. Is this a reasonable assumption? It is: we assumed that \mintinline{ocaml}{grad} would perform backpropagation over neural networks. The network over which backpropagation is performed is known at compile-time. However, notice that this justification appeals to domain-specific knowledge regarding how \mintinline{ocaml}{grad} will be used. In the general case, \mintinline{ocaml}{grad} could be applied to a function not known until runtime. It is not feasible to expect a compiler to spot all opportunities for optimisation \citep{rice-53}. In practice, while compiler engineers might have an economic incentive to write optimisations for the machine learning community, this may not be true for less lucrative domains \citep{robinson-01}. Even in machine learning, many libraries are built on top of existing languages, like \texttt{Python}, which might not perform the desired optimisations.

How does one write maintainable and efficient code, \textbf{when one cannot trust the compiler to optimise one's code}?

One answer is metaprogramming, which gives users the ability to perform code-generation. Programmers may thus take matters into their own hands: manually generating optimised code when the compiler may not automatically do so for them. The \mintinline{ocaml}{grad} function in \texttt{JAX}, a Python-based machine learning framework, uses metaprogramming for precisely this purpose \citep{jax-grad-metaprogramming}.

\subsubsection{Speeding up exponentiation with Metaprogramming}
Metaprogramming allows for code-generation via the creation and manipulation of abstract syntax trees (ASTs). I will now illustrate how metaprogramming works with reference to \texttt{MacoCaml}, which implements \textit{compile-time} metaprogramming. While the \mintinline{ocaml}{grad} example motivated metaprogramming, for pedagogical reasons, I switch to a morally equivalent, but simpler example: raising an integer \mintinline{ocaml}{x} to an exponent \mintinline{ocaml}{n}. One maintainable implementation is the \mintinline{ocaml}{pow} function (\Cref{listing:ocaml-pow-maintainable}):

\begin{code}
\begin{ocamllst}
let rec pow (n: int) (x: int) = 
  if n == 0 then 1 
  else x * pow (n-1) x
\end{ocamllst}
\captionof{listing}{A maintainable implementation of an exponentiation function}
\label{listing:ocaml-pow-maintainable}
\end{code}

\mintinline{ocaml}{pow}, which can be applied to any exponent \mintinline{ocaml}{n}, is analogous to \mintinline{ocaml}{grad_main} (\Cref{listing:ocaml-grad-main}), which could be applied to \textit{any} differentiable function $f$. 

However, should we know the exponent in advance, for example \mintinline{ocaml}{n} $ =$ \mintinline{ocaml}{2}, then a more efficient, but less maintainable implementation, is the \mintinline{ocaml}{square} function (\Cref{listing:ocaml-pow-fast}) 

\begin{code}
\begin{ocamllst}
let square x = x * x
\end{ocamllst}
\captionof{listing}{An efficient implementation of exponentiation, assuming \mintinline{ocaml}{n} $=$ \mintinline{ocaml}{2}}
\label{listing:ocaml-pow-fast}
\end{code}

\noindent \mintinline{ocaml}{square} is analogous to \mintinline{ocaml}{grad_fast} (\Cref{listing:ocaml-grad-fast}).

Metaprogramming can be utilised to write a function, \mintinline{ocaml}{pow_gen}, which resembles \mintinline{ocaml}{pow} (inheriting its maintainability), but that generates a program which resembles \mintinline{ocaml}{square} (inheriting its efficiency). \Cref{listing:ocaml-pow-gen} presents the meta-programmed \mintinline{ocaml}{pow_gen} function. Compilation generates the body of \mintinline{ocaml}{square y} (line 4), \mintinline{ocaml}{y * y * 1}. I will now explain the mechanics of generation.

\begin{code}
\begin{macocamllst}
macro rec pow_gen (n: int) (x: int expr) = 
  if n == 0 then <<1>> 
  else <<$x * $(pow_gen (n-1) x)>>
let square y = $(pow_gen 2 <<y>>) (*after compile-time: y * y * 1 *)
square 3 (*at runtime: 9*)
\end{macocamllst}
\captionof{listing}{A meta-programmed \mintinline{ocaml}{pow_gen} function, which resembles \mintinline{ocaml}{pow} but generates \mintinline{ocaml}{square}}
\label{listing:ocaml-pow-gen}
\end{code}

% Notice that \mintinline{ocaml}{pow_gen} broadly resembles \mintinline{ocaml}{pow}, with the exception of \textit{metaprogramming annotations} like \mintinline{ocaml}{<<>>} and \mintinline{ocaml}{$}. These . 
Recall that (compile-time) metaprogramming gives the programmer the ability to generate programs at compile-time, for use at run-time. We may build this in two steps, by:
\begin{enumerate}
  \item Deciding on a representation for code values, such that code can be created, and manipulated by programs. Once we have a representation for code values, it is possible to write expressions that return code values. These expressions serve as program generators.
  \item Building a mechanism for executing expressions \textit{at compile-time}. We can constrain this mechanism, using types, so only generators can be executed at compile-time. 
\end{enumerate}

First, we represent code values as ASTs. Generated programs are ASTs, and program generators are expressions that evaluate to ASTs. We can assume, for clarity, that the language offers, for each program construct, a corresponding AST node -- for example, the integer \mintinline{ocaml}{1} has AST node \mintinline{ocaml}{Int(1)}. If a program has type \mintinline{ocaml}{'a}, then its AST node has type \mintinline{ocaml}{'a expr}. One can now write program generators, that evaluate to ASTs, for example:  
\begin{macocaml}
let rec pow_gen (n: int) (x: int expr) = 
  if n == 0 then Int(1) 
  else Mul(x, (pow_gen (n-1) x))
pow_gen 2 Int(3) (*Mul(Int(3), Mul(Int(3), 1))*)
pow_gen 2 Var(y) (*Mul(Var(y), Mul(Var(y), 1))*)
pow_gen 3 Var(y) (*Mul(Var(y), Mul(Var(y), Mul(Var(y), 1)))*)
\end{macocaml}

Second, we need a mechanism to execute expressions at compile-time. In \texttt{MacoCaml}, this is the ``top-level splice'', a splice (\mintinline{ocaml}{$}) annotation not surrounded by quotes (\mintinline{ocaml}{<<>>}). For example, in \Cref{listing:ocaml-pow-gen}, there is only one top-level splice, on line 4: \mintinline{ocaml}{$(pow_gen 2 <<y>>)}. We may now shift program generators (and only program generators) under top-level splices, to perform generation at compile time. Note that to access \mintinline{ocaml}{pow_gen} at compile-time, we must also move it under the top-level splice. 
\begin{macocaml}
let square y = $(let rec pow_gen (n: int) (x: int expr) = ...
                 in pow_gen 2 Var(y)) 
               (*Mul(Var(y), Mul(Var(y), 1))*)
let cube y   = $(let rec pow_gen (n: int) (x: int expr) = ...
                 in pow_gen 3 Var(y)) 
               (*Mul(Var(y), Mul(Var(y), Mul(Var(y), 1)))*)
\end{macocaml}

To allow compile-time functions, like \mintinline{ocaml}{pow_gen}, to be re-used across multiple top-level splices, \texttt{MacoCaml} introduces the \mintinline{\macocamlLexer}{macro} (\Cref{listing:ocaml-macros})

\begin{code}
\begin{macocamllst}
macro rec pow_gen (n: int) (x: int expr) = 
  if n == 0 then Int(1) 
  else Mul(x, (pow_gen (n-1) x))
let square y = $(pow_gen 2 Var(y)) (*Mul(Var(y), Mul(Var(y), 1))*)
let cube y = $(pow_gen 3 Var(y)) (*Mul(Var(y), Mul(Var(y), Mul(Var(y), 1)))*)
  \end{macocamllst}

  \captionof{listing}{In \texttt{MacoCaml}, \mintinline{\macocamlLexer}{macro} allows for definitions to be shared across top-level splices}

  \label{listing:ocaml-macros}
\end{code}
% To allow definitions to

% The ability to create and manipulate ASTs, combined with the ability to construct and execute functions at compile-time, allows for compile-time code generation. In the example below, we execute \mintinline{ocaml}{add_zero} at compile-time to generate the program \mintinline{ocaml}{(1*2)+0}.
Further, rather than explicit AST constructors, ASTs are created by the \mintinline{ocaml}{<<>>} (``quote'') and \mintinline{ocaml}{$} annotations. Quotation creates ASTs, by converting a program into its AST representation. For example,
\[\text{\mintinline{ocaml}{<< x + 0 >>}} = \text{\mintinline{ocaml}{Plus(Var(x), Int(0))}}\]
Under a quotation, the \mintinline{ocaml}{$} annotation stops this conversion, allowing for programs that \textit{manipulate} ASTs. 
\[\text{\mintinline{ocaml}{<< $x + 0 >>}} = \text{\mintinline{ocaml}{Plus(x, Int(0))}} \]
In \texttt{MacoCaml}, the programmer interleaves quotes and splices to perform code generation
\[\text{\mintinline{ocaml}{<< $(add_zero <<1>>) + 0 >>}} = \text{\mintinline{ocaml}{Plus(add_zero Int(1), Int(0))}} \]
Notice that \mintinline{ocaml}{$} is overloaded -- we must be careful to disambiguate between ``top-level splices'', which execute programs at compile-time, and splices under quotations, which stop conversion to AST. 

Re-writing \Cref{listing:ocaml-macros} in this style (being careful about non-top-level splices), we obtain exactly \Cref{listing:ocaml-pow-gen}.

Applying this technique to the \mintinline{ocaml}{grad} example, we obtain
\begin{macocaml}
macro rec grad_gen f x = match f with
  | Sin
  | Tanh
  | Sigmoid
  | ...
  | Polynomial(_) -> grad_of f x 
  | Compose(f, g) -> <<$(grad_gen f x) * $(grad_gen g (app f x))>>

let grad_fast x = $(grad_gen Compose(Tanh, Sin) <<x>>)
\end{macocaml}
whathere \mintinline{ocaml}{grad_of} and \mintinline{ocaml}{app} are appropriately modified.
\subsection{The Design Space of Metalanguages}\label{subsection:metaprogramming-design}
Different metalanguages provide slightly different variants of metaprogramming to the user. In this section, I broadly taxonomise these languages by considering three key design decisions:
\begin{enumerate}
  \item \textbf{\textsf{Homogenous or Heterogenous}}\\
         \textit{Do the generated (``object'') and generating (``meta'') languages agree or differ?}
  
        If the object and meta languages are the same, this is known as {homogenous} metaprogramming. Otherwise, it is {heterogenous} \citep{kiselyov-2024}.

        \texttt{MacoCaml} allows for homogenous metaprogramming, where \texttt{OCaml} code generates \texttt{OCaml} code. In contrast, \texttt{MetaHaskell} \citep{mainland-2012} programs generate \texttt{C} code, allowing for heterogenous metaprogramming. 

  \item \textbf{\textsf{Run-time or Compile-Time}} \\
        \textit{When does the generation take place?}

        Code generation could take place at compile-time (as with \texttt{MacoCaml} programs or \texttt{C} macros), or at run-time (as with \texttt{MetaOCaml} \citep{kiselyov-14}). 

        % Run-time code generation may still be used to write fast and maintainable code, by adding a phase after the traditional compilation pipeline where the compiled code is executed. 
        
        Run-time and compile-time metaprogramming differ non-trivially. Run-time metaprogramming requires a language construct (\mintinline{ocaml}{!}, or ``run'') for explicit invocation of the compiler. Further, in run-time metaprogramming, generated and generating programs may share a heap. 

        \texttt{MacoCaml} supports compile-time metaprogramming, and we will pay no further attention to run-time metaprogramming.
        
  \item \textbf{\textsf{Two-stage or Multi-stage}} \\
  \textit{How many stages of code generation are allowed?}

  When introducing \texttt{MacoCaml}, I illustrated how one uses top-level splices to perform shift computation from run-time (``level 0'') to compile-time (``level -1''). Might it be possible to shift computation from compile-time to a pre-compile-time (``level -2'' phase), for example, via a nested splice?
  \begin{macocaml}
$($ pow_gen 2 Var(y))
  \end{macocaml}
  In a two-stage system, one is restricted to operating between two levels, so this is disallowed. In contrast, in a multi-stage system, one can operate between any number of levels. Multi-stage metaprogramming is thus strictly more general than two-stage metaprogramming.
  
  Although nested splices are disallowed in \texttt{MacoCaml}, it is a multi-stage system, since entire modules may be imported at a decremented level \citep{xie-2023}. 
\end{enumerate}
  
\texttt{MacoCaml} offers homogenous, compile-time, multi-stage metaprogramming. The scope of this dissertation is slightly more restrictive: I focus on two-stage, not multi-stage metaprogramming. This restriction was motivated by a cost-benefit analysis:
\begin{enumerate}
  \item \textbf{Cost}: Since in \texttt{MacoCaml}, the module system is the only mechanism for achieving multi-stage programming, investigating multi-stage metaprogramming would require the investigation of module systems, effects, and metaprogramming. The interaction between module systems and metaprogramming is still an ongoing area of research \citep{chiang-2024}.
  \item \textbf{Benefit}: In practice, ``almost all uses'' of multi-stage metaprogramming only use two stages \citep{inoue-2012}. Further, scope extrusion can be observed, and is often studied, in two-stage systems \citep{isoda-24,kiselyov-16}.
\end{enumerate}
% \subsubsection{Quasi-quotation vs Code combinators}
% A second key design decision relates to how ASTs are constructed. Quasi-quotation 

% \subsubsection{Run-Time vs Compile-Time}

% \subsubsection{Two-Stage vs Multi-Stage}

% \subsection{MacoCaml}\label{subsection:metaprogramming-macocaml}

\section{Effect Handlers}\label{section:effects-technical}
What is an effect handler? I will first motivate effect handlers by considering the problem of adding resumable exceptions to \texttt{OCaml} (\Cref{subsection:effect-handler-motivation}). Second, I will introduce a fine-grained call-by-value calculus for studying the operational behaviour of effect handlers, Ã  lÃ¡ \citet{pretnar-15} (\Cref{subsection:effect-handler-calculus}). This calculus will be useful both for precise description of effect handlers, and as a basis for investigating the interaction between metaprogramming and effect handlers (once the calculus has been extended with metaprogramming facilities). Finally, since different design decisions for effect handlers could affect the nature of their interaction with metaprogramming, I will consider the design space of effect handlers (\Cref{subsection:effect-handler-design}).

\subsection{Composable and Customisable Effects}\label{subsection:effect-handler-motivation}
Effects are a mechanism by which a program interacts with its environment. Examples of effects include state, (resumable) exceptions, non-determinism, and I/O. Effects are typically defined and understood separately, meaning they are not easily composable. They are also implemented by compiler engineers rather than programmers, meaning they are not customisable. Effect handlers provide a programmable, unifying framework that may be instantiated into different effects. This allows for composable and customisable treatment of effects.

To illustrate the need for effect handlers, consider the following problem, by \citet{kiselyov-2012}. Assume a binary search tree of (key, value) pairs. The following code provides two functions. The first \mintinline{ocaml}{find}s a value \mintinline{ocaml}{v} associated with key \mintinline{ocaml}{k}, raising a \mintinline{ocaml}{NotFound} exception if \mintinline{ocaml}{k} is not in the tree. The second \mintinline{ocaml}{update}s the dictionary with a fresh key value pair, overwriting old values.

\begin{ocaml}
type ('a, 'b) tree = Lf | Br of 'a * 'b * tree * tree 

let rec find (t: tree) (k: 'a) = match t with 
 | Lf -> raise NotFound()
 | Br(k', v, l, r) -> if k == k' then v 
                      else if k < k' then find l k
                           else find r k

let rec update (t: tree) (k: 'a) (v: 'b) = match t with 
  | Lf -> Br(k, v, Lf, Lf)
  | Br(k', v', l, r) -> if k == k' then Br(k, v, l, r)
                       else if k < k' then Br(k', v', update l k v, r)
                            else Br(k', v', l, update r k v)
\end{ocaml}

Assume the task is to build a \mintinline{ocaml}{findOrInsert} function that either finds the value associated with a key, \textit{or} inserts a default value. A naÃ¯ve approach to writing this function would be
\begin{ocaml}  
let rec findOrInsert (t: tree) (k: 'a) (default: 'b) = 
  try find t k with NotFound -> insert t k default
\end{ocaml}
This function is \textbf{inefficient}. If a \mintinline{ocaml}{NotFound} exception is raised, then the \mintinline{ocaml}{find} function will have raised the exception at the point where the default value should be inserted. The function could be twice as efficient if the exception could be resumed at the point where the exception was raised, in the following style.
\begin{ocaml}
let rec findOrInsert (t: tree) (k: 'a) (default: 'b) = 
  try find t k with NotFound(p) -> continue p Br(k, default, Lf, Lf)
\end{ocaml}
\mintinline{ocaml}{p} represents the suspended program to be resumed, and is known as a \textit{delimited continuation}.

The aforementioned problem motivates the need for resumable exceptions. To understand the need for effect handlers, consider how one might go about \textbf{implementing} resumable exceptions. One approach might be to fork the implementation of handlers and tweak it ever-so-slightly. This solution does not scale well. First, the solution may not be \textbf{composable}. The intended informal semantics for resumable exceptions is ``effectively equivalent to exceptions, with the additional power to resume programs''. Resumable exceptions should thus interact with other exceptions in a predictable way, but this is difficult to guarantee -- and \textit{continually} guarantee -- especially as implementations evolve, and more variants of exceptions are demanded. Second, the solution is not \textbf{customisable}. To add resumable exceptions requires a compiler engineer to modify the compiler. With the exception of raising an issue, there is nothing the programmer may do, in the moment, to meet their need.

Effect handlers resolve both composability and customisability issues. Much like how exception handlers allow users to create custom exceptions with custom semantics, effect handlers provide a general framework for creating custom effects with custom semantics. The interaction between effect handlers is described abstractly, parameterising over the exact semantics of the effect. Hence, implementing effects -- in the earlier example, resumable exceptions, but more generally, state, I/O, greenthreading, non-determinism, and more -- as effect handlers ensures composability by design. 

With effect handlers, we can re-write the previous example to obtain the behaviour of resumable exceptions, even if the \texttt{OCaml} compiler does not support it, with the guarantee that \mintinline{ocaml}{NotFound} will interact predictably with other defined effects.
\begin{ocaml}
type _ Effect.t += NotFound: unit -> tree t

let rec find (t: tree) (k: 'a) = match t with 
  | Lf -> NotFound()
  | Br(k', v, l, r) -> if k == k' then v 
                      else if k < k' then find l k
                            else find r k

let rec findOrInsert (t: tree) (k: 'a) (default: 'b) = 
  match find t k with NotFound(p) with 
  | v -> v
  | effect NotFound k -> continue p Br(k, default, Lf, Lf)
\end{ocaml}
Since effect handlers may be instantiated into a range of different effects, considering the interaction of metaprogramming with effect handlers is an exercise in killing many birds with a single stone. Additionally, effect handlers were recently added to \texttt{OCaml} \citep{sivaramakrishnan-21}, making their interaction a timely problem. 
\subsection{\efflang{}: A Calculus for Effect Handlers}\label{subsection:effect-handler-calculus}
\newcommand{\print}[1]{\texttt{\textbf{print}(#1)}}
\newcommand{\readInt}[1]{\texttt{\textbf{read\_int}(#1)}}


Having motivated effect handlers, I will now describe a calculus, which I call \efflang, for reasoning about their operational behaviour.\ \efflang{} is a slight variant of the calculus described by \citet{pretnar-15}. Understanding \efflang{} will be useful for two reasons. First, a precise description of the operational behaviour of effects will aid reasoning about their interaction with metaprogramming. Second, my universal calculus will be described by extending \efflang. Throughout this section, I will use the \efflang{} program in \Cref{listing:efflang-running-example} as a running example. 

\begin{code}
  \begin{efflst}
    $\begin{array}{l}
      \textbf{\texttt{handle}} \\
      \quad \bind{x}{\print{1};\return{\texttt{1}}}{{\bind{y}{\print{2};\return{2}}{\, x + y}}} \\
      \textbf{\texttt{with}} \\
      \quad \{ \textbf{\texttt{return}}(x) \mapsto {\return{(x, \texttt{""})}}; \\
      \quad \; \, \textbf{\texttt{print}}(x, k) \mapsto {\bind{(v, s)}{\continue{k}{()}}{\return{(v, \texttt{f"\{$x$\};"} \text{ \^\ } s)}}}\}\\
    \end{array}$

    \vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{(\texttt{3}, \texttt{"1;2"})}\end{array}$}

  \end{efflst}
  \captionof{listing}{An \efflang{} program that returns $(\texttt{3}, \texttt{"1;2"})$. It will be used as a running example throughout this section.}
  \label{listing:efflang-running-example}
  \end{code}

\Cref{fig:eff-lang-syntax} collates the base syntax of \efflang. In addition to this base syntax, in this section, I will assume \efflang{} is extended with the following language extensions: a unit value $()$, pairs $(\texttt{1}, \texttt{2})$ which can be destructured $\bind{(x, y)}{\return{(\texttt{1}, \texttt{2})}}{\, x+y}$, strings \texttt{"Hello"}, format strings \texttt{f"\{1\}"}, and string concatenation \texttt{\^}. For example, the following code evaluates to \texttt{"Revolution 9"}.

% \texttt{\^} \texttt{f"\{$y$\}"}}}
\begin{eff}
$\begin{array}{l}\bind{(x, y)}{\return{(\texttt{"Revolution"}, \texttt{f"\{9\}"})}}{\, x \text{ \^\ } y}\end{array}$
\vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{\texttt{"Revolution 9"}}\end{array}$}
\end{eff}
Further, I use $c_1;c_2$ as syntactic sugar for $\bind{\_}{c_1}{c_2}$. 
I will explain key language constructs in turn.
\begin{figure}[t]
  \begin{eff-desc}

  $\begin{array}{llll}
  \text{Values} & v & := & x \mid n \mid \lambda x. c \mid \kappa x.c \mid h \\

  \text{Computations} & c & := & v_1 v_2 \mid \return{v} \mid \bind{x}{c_1}{c_2} \mid \\
                             &&& \op{v} \mid \handleWith{c}{h} \mid \continue{v_1}{v_2} \\ 
  \text{Handlers} & h & := &\returnHandler{x}{c} \mid h;\opHandler{x}{k}{c}
  \end{array}$
  
  \end{eff-desc}
  \caption{The syntax of \efflang. Terms are syntactically divided into values $v$ and computations $c$ }
  \label{fig:eff-lang-syntax}
\end{figure}

\subsubsection{Sequencing computations: \texttt{do} and \texttt{return}}
Effects force us to carefully consider the order of evaluation. For example, consider the following \texttt{OCaml} programs
\begin{ocaml}
let pure      = (1+0) + (2+0)
let effectful = let l = new 0 in (l := 1; 1) + (l := 2; 2)
\end{ocaml}
The result of \mintinline{ocaml}{pure}, which has no effects, is independent of the evaluation order. In contrast, the result of \mintinline{ocaml}{effectful} \textit{is} dependent on the evaluation order. If terms are evaluated left-to-right, the value of \mintinline{ocaml}{!l} is \mintinline{ocaml}{2}, otherwise, it is \mintinline{ocaml}{1}.

In order to be precise about the order of evaluation, \efflang{} terms are stratified into two syntactic categories, ``inert values'' $v$ and ``potentially effectful computations'' $c$ \citep{pretnar-15}. $\return{v}$ lifts values into computations, and is also the result of fully evaluating a computation. $\bind{x}{c_1}{c_2}$ sequences computations, forcing programmers to be explicit the order of evaluation.  First, $c_1$ is fully evaluated to obtain some $\return{v}$. The value $v$ is then bound to $x$, and finally $c_2$ is evaluated. 

For example, extending \efflang{} with a \texttt{plus} function, what is the order of evaluation of \texttt{plus} \texttt{1} \texttt{2}? Do we evaluate both arguments before applying them, or interleave evaluation and application? The syntax forces programmers to choose explicitly. We can either fully evaluate both arguments before applying them in turn, 
\begin{eff}
$\bind{x}{\return{\texttt{1}}}{(\bind{y}{\return{\texttt{2}}}{(\bind{f}{\texttt{plus} \, x}{f y})})}$
\end{eff}
or alternatively, evaluate \texttt{1}, apply it, then evaluate \texttt{2}
\begin{eff}
$\bind{x}{\return{\texttt{1}}}{(\bind{f}{\texttt{plus} \, x}{\bind{y}{\return{\texttt{2}}}{f y}})}$
\end{eff}

Both choices are valid, but the programmer must choose. For clarity, where the ordering cannot affect the result (both of the aforementioned choices evaluate to $\return{3}$), I will abuse notation and write (for instance) $1+2$. 

\subsubsection{Performing effects: \texttt{op}, \texttt{handle}, and \texttt{continue}}
Having made explicit the order of operation, we may now add effect handlers. Recall that effect handlers allow users to register custom effects with custom semantics. I will now illustrate how this is supported by \efflang{}.

For simplicity, \efflang{} assumes that the effects have been registered in advanced, parameterising over them with the placeholder $\op{v}$. Assume that the user has declared the effects \texttt{\textbf{print}} and \texttt{\textbf{read\_int}} in advance. This would allow the user to write programs like 
\begin{eff}
$\bind{x}{\print{1};\return{\texttt{1}}}{{\bind{y}{\print{2};\return{\texttt{2}}}{\, x + y}}}$
\end{eff}

In the program fragment above, we know that \textbf{\texttt{print}} is an effect, but we do not know its semantics. Effect handlers, which comprise a \textbf{return handler} and zero or more \textbf{operation handlers}, specify how effects interact with their environment, and thus may be used to give effects meaning. I will define an effect handler that accumulates print statements in a string (some ``\texttt{stdout}'').  For example, the aforementioned program should return $(\texttt{3}, \texttt{"1; 2"})$.

We begin by considering how to handle the case where there are no calls to \textbf{\texttt{print}}. For example, in the program $\return{\texttt{3}}$. We may wish to return both the value, and the empty string (empty \texttt{stdout}) to the environment: in this case, $(\texttt{3}, \texttt{""})$. We can achieve this by specifying a \textit{return handler}.
\[\textbf{\texttt{return}}(x) \mapsto {c}\]
In this case, we set $c$ to $\return{(x, \texttt{""})}$. All effect handlers must specify a return handler. In many cases, the return handler is simply the identity ($c$ is set to $\return{x}$): for brevity and clarity, if the return handler is the identity, I may drop it. 

Next, we consider how to handle a call to \textbf{\texttt{print}}. We use an operation handler of the form 
\[\textbf{\texttt{print}}(x, k) \mapsto {c}\]
Where $c$ is the user-defined semantics for \textbf{\texttt{print}}.
Concretely, one instance of $c$ is 
\[\textbf{\texttt{print}}(x, k) \mapsto {\bind{(v, s)}{\continue{k}{()}}{\return{(v, \texttt{f"\{$x$\};"} \text{ \^\ } s)}}}\]
In the definition of $c$, the programmer may refer to $x$ and $k$, which I will now explain. $x$ allows programs to send values (for example, values to be printed) to their environment. $k$ is a delimited continuation representing a suspended program, awaiting a value from the environment. Effects also allow programs to receive data from their environment, as in
\[1 + \textbf{\texttt{get\_int\_from\_user}}()\]
Note that the program is suspended until the value is receive. We may write the suspended program as $1 + [-]$, where $[-]$ indicates an as-yet-unknown value. This suspended program is represented by the continuation $k$. The expression 
\[\continue{k}{v}\] 
is used to resume the suspended program with value $v$. 

We are now able to interpret the concrete operation handler $c$: we resume the suspended program, supplying a unit value, since \textbf{\texttt{print}} effects do not receive values from their environment. This returns a value $v$ and some partially accumulated \texttt{stdout} $s$. We prepend the printed value, $x$, onto $s$. 
% Second, \textit{any effect can be understood in terms of send and receive operations}. Programs interact with their environment in two fundamental ways: they either \textit{receive} values from their environment (as above), or they \textit{send} values to its environment, for example via a \texttt{print}. 

% $x$, $k$, 

% where $x$ is a value sent by the program to its environment, $k$ is the program that was suspended on encountering the effect (a delimited continuation), and $c$ is a user-defined computation that specifies the semantics of \textbf{\texttt{print}}. 
% In this example, we assume that programs evaluate to tuples $(\texttt{value}, \texttt{std\_out})$. Programs may receive values from their environment: we use the syntax $\continue{k}{()}$ to give the program the unital value. Programs may also send values $x$, which are processed by the environment -- in this case, prepended to the \texttt{std\_out}. If \textbf{\texttt{print}} is given these semantics, the earlier program should return $(\texttt{3}, \texttt{"1; 2"})$.

% There is a slight complication. Our assumption is wrong: the program evalues $\return{\texttt{3}}$ -- we need to \textit{initialise} \texttt{std\_out}. This is done using a little boiler plate. Every effect handler must additionally define a \textbf{\texttt{return}} handler, which treats \textbf{\texttt{return}} as a special effect, whose semantics we may modify 
% 
Having defined the semantics for \textbf{\texttt{print}}, the user may now interpret the earlier example with their semantics, using the $\handleWith{e}{h}$ construct. Doing so results in the program in \Cref{listing:efflang-running-example}.

Notice that multiple effects may be handled by the same handler, and the same effect might be handled by multiple handlers, potentially with different semantics. 

\subsubsection{Operational Semantics}
Having described informally the desired semantics of \efflang{}, we may now make our intuitions precise, by means of an operational semantics. The operational semantics is collated in \Cref{fig:efflang-opsem}. 

\begin{figure}[ht]
  \arraycolsep=3pt
\begin{eff-desc}
  
  \renewcommand{\effconfiguration}[2]{{#1}; {#2}}
  \renewcommand{\transition}[2]{#1 & \leadsto & #2}
  \newcommand{\rulename}[2]{(\textsc{{#1}-{#2}})}
  \newcommand{\reductionRule}[1]{\rulename{Red}{#1}}
  \newcommand{\congruenceRule}[1]{\rulename{Cng}{#1}}
  \newcommand{\effectRule}[1]{\rulename{Eff}{#1}}
  \footnotesize
  \textbf{Auxiliary Definitions}
  {\scriptsize
    \[\begin{array}{lrcl}
    \text{Evaluation Frame } & F & ::= & \bind{x}{-\,}{c_2} \mid \handleWith{-}{h} \\
    \text{Evaluation Context } & E & ::= & [] \mid E::F \\ \vspace{1mm} \\
    \text{Domain of Handler} & \textsf{dom}(h) & \triangleq & \textsf{dom}(\returnHandler{x}{c}) = \emptyset, \\
    &&&\textsf{dom}(h;\opHandler{x}{k}{c}) = \textsf{dom}(h) \cup \{ \textbf{\textsf{op}} \} \\  
    \vspace{1mm} \text{Handled Effects} & \textsf{handled}(E) & \triangleq & \textsf{handled}([]) = \emptyset, \\ 
    &&& \textsf{handled}(E::\bind{x}{-\,}{c_2}) = \textsf{handled}(E), \\
    &&& \textsf{handled}(E::\handleWith{-}{h}) = \textsf{handled}(E) \cup \textsf{dom}(h),
  \end{array}
  \]}

\noindent\textbf{Operational Semantics}
  {\scriptsize
\[
  \begin{array}{rrcl}
  \reductionRule{App} & \transition{\effconfiguration{(\function{x}{c})v}{E}}{\effconfiguration{c[v/x]}{E}}\\
  \reductionRule{Seq} & \transition{\effconfiguration{\bind{x}{\return{v}}{c}}{E}}{\effconfiguration{c[v/x]}{E}}\\
  \reductionRule{Hdl} & \transition{\effconfiguration{\handleWith{\return{v}}{h}}{E}}{\effconfiguration{c[v/x]}{E}} \quad (\text{where $\returnHandler{x}{c} \in h$)}\\
  \vspace{1mm} \\ 
  \congruenceRule{Psh} & \transition{\effconfiguration{F[c]}{E}}{\effconfiguration{c}{E::F}} \\
  \congruenceRule{Pop} & \transition{\effconfiguration{\return{v}}{E::F}}{\effconfiguration{F[v]}{E}}\\
  \vspace{1mm} \\
  \effectRule{Op} & \transition{\effconfiguration{\op{v}}{E_1 @ [h] @ E_2}}\effconfiguration{c[v/x, \kappa x. \, \handleWith{E_2[\return{x}]}{h} / k]}{E_1}\\
  &&& \text{(where $\opHandler{x}{k}{c} \in h$ and $\textbf{\textsf{op}} \notin \textsf{handled}(E_2)$)}\\
  \effectRule{Cnt} & \transition{\effconfiguration{\continue{E_2}{v}}{E_1}}{\effconfiguration{\return{v}}{E_1 @ E_2}}

\end{array}
\]
  }
\end{eff-desc}
\caption{The operational semantics of \efflang. The semantics is given on configurations of the form $\langle c, E \rangle$, with the brackets dropped for clarity. Rules are divided into three classes: reduction rules $\textsc{Red-}X$, which perform computation, congruence rules $\textsc{Cng-}Y$ which manipulate the evaluation context, and effect rules $\textsc{Eff-}Z$ that are special to \efflang}
\label{fig:efflang-opsem}
\end{figure}

The operational semantics is given on configurations of the form $\effconfiguration{c}{E}$, where $c$ is a term and $E$ is an evaluation context, in the style of \citet{felleisen-87}. Evaluation contexts are represented as a stack of evaluation frames $F$, Ã  lÃ¡ \citet{kiselyov-2012}. For clarity, I will compress multiple stack frames using nesting. For example, instead of $\bind{x}{-\,}{c_2} :: \bind{y}{-\,}{c_1}$, I will write $\bind{x}{(\bind{y}{-\,}{c_1})}{c_2}$. Most of the rules are standard. We will focus on two rules: \textsc{Eff-Op}, the mechanism for giving effects custom semantics, and \textsc{Eff-Cnt}, the mechanism for resuming programs. 

To illustrate the operation of \textsc{Eff-Op} and \textsc{Eff-Cnt}, consider the evaluation of the running example in \Cref{listing:efflang-running-example}, beginning with an empty context. Let $h$ be the handler body 
\[\begin{array}{l}\{ \textbf{\texttt{return}}(x) \mapsto {\return{(x, \texttt{""})}}; \\
\; \, \textbf{\texttt{print}}(x, k) \mapsto {\bind{(v, s)}{\continue{k}{()}}{\return{(v, \texttt{f"\{$x$\};"} \text{ \^\ } s)}}}\}\end{array}\]
After several applications of \textsc{Cng-Pop}, we obtain the configuration
{  \arraycolsep=3pt
\small
\[\begin{array}{rcl}
  \langle \, \print{1} &;& \handleWith{-}{h} :: \\
                        && \bind{x}{-;\return{\texttt{1}}}{{\bind{y}{\print{2};\return{\texttt{2}}}{\, x + y}}} \, \rangle
\end{array}
  \]
}

Let $E = \bind{x}{\return{u};\return{\texttt{1}}}{{\bind{y}{\print{2};\return{\texttt{2}}}{\, x + y}}}$. Applying \textsc{Eff-Op}, we can suspend the program, find the handler $h$ with the user's semantics for \textbf{\texttt{print}}, and give the \textbf{\texttt{print}} effect the desired semantics
{  \arraycolsep=3pt
\small
\[\begin{array}{rcl}
  \langle \, \bind{(v, s)}{\continue{(\continuation{u}{E})}{()}}{\return{(v, \texttt{f"\{$1$\};"} \text{ \^\ } s)}} &;& \handleWith{-}{h} \, \rangle
\end{array}
  \]
}
Applying \textsc{Cng-Pop},
{  \arraycolsep=3pt
\small
\[\begin{array}{rcl}
  \langle \, \continue{(\continuation{u}{E})}{()} &;& \handleWith{-}{h} :: \\
                              && \bind{(v, s)}{-\,}{\return{(v, \texttt{f"\{$1$\};"} \text{ \^\ } s)}} \, \rangle
\end{array}
  \]
}
Applying \textsc{Eff-Cnt}, we can resume the program that was suspended
{  \arraycolsep=3pt
\small
\[\begin{array}{rcl}
  \langle \, \return{()} &;& \handleWith{-}{h} :: \\
                              && \bind{(v, s)}{-\,}{\return{(v, \texttt{f"\{$1$\};"} \text{ \^\ } s)}} \\
                              && \bind{x}{-;\return{\texttt{1}}}{{\bind{y}{\print{2};\return{\texttt{2}}}{\, x + y}}} \, \rangle
\end{array}
  \]
}
The side-condition on \textsc{Eff-Op} is needed because the user may define multiple handlers with different semantics for the same effect. The side-condition resolves any ambiguity by using the \textit{latest} handler. For example, the following program has a \textbf{\texttt{read}} effect that is given two definitions: it could read either \texttt{1} or \texttt{2}. The ambiguity is resolved by choosing the latest handler: in this case, \texttt{1}.
\begin{eff}
$\begin{array}{l}
  \textbf{\texttt{handle}} \\
  \quad \handleWith{\textbf{\texttt{read}()}}{\returnHandler{y}{\return{y}}; \textbf{\texttt{read}}(x, k) \mapsto {\continue{k}{1}}} \\
  \textbf{\texttt{with}} \\ 
  \quad \{ \returnHandler{y}{\return{y}}; \textbf{\texttt{read}}(x, k) \mapsto {\continue{k}{2}} \}
\end{array}$ 

\vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{\texttt{1}}\end{array}$}
\end{eff}


\subsubsection{Type-and-Effect System}
We now give a type-and-effect system to \efflang{}. \Cref{fig:efflang-type-syntax} collates the syntax of \efflang{} types, which I will now briefly describe. 

\begin{figure}
  \begin{eff-desc}
  $\begin{array}{llllr}
    \text{Effects row} & \Delta & ::= & \emptyset \mid \Delta \cup \{ \texttt{op}_i \} \\ \\
    \text{Value type} & T & ::= & \mathbb{N} & \\
                              &&& \mid \functionType{T_1}{T_2} & \text{functions}\\
                              &&& \mid \handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}} & \text{handlers} \\
                              &&& \mid \continuationType{T_1}{T_2} & \text{continuations}\\ \\
    \text{Computation type} & \effectType{T}
  \end{array}$
  \end{eff-desc}
  \caption{\efflang{} types. Notice that, just as terms are divided into values and computations, types are divided into value types ($T$) and computation types ($\effectType{T}$)}
  \label{fig:efflang-type-syntax}
\end{figure}

Syntactically, just as terms are divided into values and computations, types are divided into value types (for example, $\mathbb{N}$) and computation types (for example, $\effectType[\{\textbf{\texttt{print}} \}]{\mathbb{N}}$). Since computations may have effects, computation types track unhandled effects using an effects row (typically labelled $\Delta$), which in this system is simply a set. This type-and-effect system allows us to distinguish between values, computations that return values, and computations that return values and additionally have some unhandled side effects.
\[
\begin{array}{lll}
  \textbf{Term} &\hspace{8mm}& \textbf{Type} \\
  3 && \mathbb{N} \\ 
  \bind{x}{\return{1}}{\bind{y}{\return{2}}{x + y}} &&  \effectType[\emptyset]{\mathbb{N}} \\ 
  \bind{x}{\texttt{\textbf{print}($1$)}; \return{1}}{\bind{y}{\return{2}}{x + y}} &&  \effectType[\{ \texttt{\textbf{print}}\}]{\mathbb{N}} \\ 
\end{array}
\]

Functions are values, and are applied to other values, but produce computations on application. For example, the function 
\[
\function{x:\mathbb{N}}{\, \textbf{\texttt{print}}(x); \return{x}}
\]
is a value that accepts a value of type $\mathbb{N}$ and returns a computation of type $\effectType[\{\textbf{\texttt{print}} \}]{\mathbb{N}}$. We thus say functions have suspended effects, which we write $\functionType{T_1}{T_2}$. In this case, the function has type $\functionType[\{ \textbf{\texttt{print}} \}]{\mathbb{N}}{\mathbb{N}}$. For technical reasons, continuations and functions need to be distinguished, but in most cases they may be treated equivalently. 

Handlers transform computations of one type to computations of another type. This happens in two ways: first, by handling effects, and thus removing them from the effects row (which recall represents unhandled effects). Second, by modifying the return type of computations. To reflect both abilities, handlers are given a type of the form $\handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}}$. For example, a handler of the form  
\[ \begin{array}{ll}
  \quad \{ \textbf{\texttt{return}}(x) \mapsto {\return{(x, \texttt{""})}}; \\
    \quad \, \,\, \textbf{\texttt{print}}(x, k) \mapsto {\bind{(v, s)}{\continue{k}{()}}{\return{(v, \texttt{f"\{$x$\};"} \text{ \^\ } s)}}}\}
  \end{array}
\]
may be given type $\handlerType{\effectType[\{ \textbf{\texttt{print}} \}]{\mathbb{N}}}{\effectType[\emptyset]{(\mathbb{N} \times \textsf{String})}}$, reflecting both the handling of the \textbf{\texttt{print}} effect and the transformation of the return type to include the collated print statements.

I now consider the typing rules for terms, which are collated in \Cref{fig:efflang-type-system}. Most rules are standard, but a few are worth paying attention to. 

\begin{figure}[t]
  \begin{eff-desc}
    \centering 
    \begin{minipage}[t]{0.2\textwidth}
      \centering
      $\inferrule[(Nat)]
      { \\ }
      {\type{n}{\mathbb{N}}}$
      \end{minipage}% 
  \begin{minipage}[t]{0.2\textwidth}
    \centering
  $\inferrule[(Var)]
  {\Gamma(x) = T}
  {\type{x}{T}}$
  \end{minipage}% 
  \begin{minipage}[t]{0.3\textwidth}
    \centering
  $\inferrule[(Lambda)]
    {\type[, x:T_1]{c}{\effectType{T_2}}}
    {\type{\function{x}{c}}{\functionType{T_1}{T_2}}}$
  \end{minipage}%
  \begin{minipage}[t]{0.3\textwidth}
  \centering
$\inferrule[(Continuation)]
  {\type[, x:T_1]{c}{\effectType{T_2}}}
  {\type{\continuation{x}{c}}{\continuationType{T_1}{T_2}}}$
\end{minipage}
  
  \vspace{5mm}
  
  \begin{minipage}[t]{0.5\textwidth}
    \centering
  $\inferrule[(App)]
    {\type{v_1}{T_1 \oset{\text{\tiny{$\Delta$}}}\longrightarrow T_2} \\ \type{v_2}{T_1}}
    {\type{v_1 \, v_2}{\effectType{T_2}}}$
  \end{minipage}%
  \begin{minipage}[t]{0.5\textwidth}
    \centering
  $\inferrule[(Continue)]
    {\type{v_1}{\continuationType{T_1}{T_2}} \\ \type{v_2}{T_1}}
    {\type{\continue{v_1}{v_2}}{\effectType{T_2}}}$
  \end{minipage}

  \vspace{5mm}

  \begin{minipage}[t]{0.3\textwidth}
    \centering
  $\inferrule[(Return)]
    {\type{v}{T}}
    {\type{\return{v}}{\effectType{T}}}$
  \end{minipage}%
  \begin{minipage}[t]{0.55\textwidth}
    \centering
  $\inferrule[(Do)]
    {\type{c_1}{\effectType{T_1}} \\ \type[, x: T_1]{c_2}{\effectType{T_2}}}
    {\type{\bind{x}{c_1}{c_2}}{\effectType{T_2}}}$
  \end{minipage}
  
  \vspace{5mm}
  % \begin{minipage}[t]{0.5\textwidth}
  %   \centering
  % $\inferrule[(Do)]
  %   {\type{c_1}{\effectType{T_1}} \\ \type[, x: T_1]{c_2}{\effectType{T_2}}}
  %   {\type{\bind{x}{c_1}{c_2}}{\effectType{T_2}}}$
  % \end{minipage}%
  \begin{minipage}[t]{0.5\textwidth}
    \centering
  $\inferrule[(Op)]
    {\type{v}{T_1} \\ \texttt{op}: T_1 \rightarrow T_2 \in \Sigma \\ \texttt{op} \in \Delta}
    {\type{\op{v}}{\effectType{T_2}}}$
  \end{minipage}%
  \begin{minipage}[t]{0.5\textwidth}
    \centering
  $\inferrule[(Handle)]
    {\type{c}{\effectType{T_1}} \\ \type{h}{\handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}}} \\ \forall \textsf{op} \in \Delta \setminus \Delta'. \textsf{op} \in \textsf{dom}(h)}
    {\type{\handleWith{c}{h}}{\effectType[\Delta']{T_2}}}$
  \end{minipage}\\

  \vspace{5mm}

  \begin{minipage}[t]{\textwidth}
    \centering
  $\inferrule[(Ret-Handler)]
    {\type[, x:T_1]{c}{\effectType[\Delta']{T_2}}}
    {\type{\returnHandler{x}{c}}{\handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}}}}$
  \end{minipage}
  
  \vspace{5mm}
  % \[\inferrule [(Lam)]
  % {\type[, x:T_1]{t}{\sEffect{T_2}}}
  % {\type{\function{x}{t}}{\sFunctionType{T_1}{T_2}}}\]
  
  % \[\inferrule[(App)]
  % {\type{n_1}{\sFunctionType{T_1}{T_2}} \\
  \begin{minipage}[t]{\textwidth}
    \centering
  $\inferrule[(Op-Handler)]
    { \texttt{op}: A \to B \in \Sigma \\\\ 
      \type{h}{\handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}}}\\
      \type[, x:A, k:{\continuationType[\Delta']{B}{T_2}} ]{c}{\effectType[\Delta']{T_2}}\\
      \Delta' \subseteq \Delta \setminus \{ \texttt{op} \} \\
             \opHandler{x'}{k'}{c'} \notin h}
    {\type{h ; \opHandler{x}{k}{c}}{\handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}}}}$
  \end{minipage}
  \end{eff-desc}
  \caption{Typing rules for \efflang{} terms}
  \label{fig:efflang-type-system}
  \end{figure}

First, the \textsc{Return} and \textsc{Do} rules. In the \textsc{Return} rule, we are allowed to assign the term $\return{v}$ any set of effects. For example, we could write:
\[\inferrule{ }{\type[]{\return{\texttt{0}}}{\effectType[\{ \textbf{\texttt{print}} \}]{\mathbb{N}}}}\] 
This flexibility is important, because to type $\bind{x}{c_1}{c_2}$, the \textsc{Do} rule requires both $c_1$ and $c_2$ to have the same effects. For example, without this flexibility, we would not be able to complete the following typing derivation
\[\inferrule{\vdots}{\type[]{\bind{x}{\print{0}}{\return{\texttt{0}}}}{\effectType[\{ \textbf{\texttt{print}} \}]{\mathbb{N}}}}\] 
A valid alternative would be to forbid this flexibility and add explicit subtyping. However, such an approach would no longer be syntax directed. 

Second, the \textsc{Op} rule. Previously, we assumed that the user declared their effects in advance. We also assume that they declare the types of their effects in advance, and that we we store the mapping from effects to types in $\Sigma$. For example, we might assume $\Sigma = \{ \textbf{\texttt{print}}: \mathbb{N} \to 1 \}$. In \texttt{OCaml}, this would correspond to writing:
\begin{ocaml}
type _ Effect.t += Print: nat -> unit
\end{ocaml}
Note further the $\texttt{op} \in \Delta$ restriction -- flexibility allows us to over-approximate the effects in a term, but never underapproximate them. 

Third, the \textsc{Ret-Handler} and \textsc{Op-Handler} rules, which are used to type handlers, which I will explain by means of an example. Assume we are trying to type the handler 
\[ \begin{array}{ll}
  \quad \{ \textbf{\texttt{return}}(x) \mapsto {\return{(x, \texttt{""})}}; \\
    \quad \, \,\, \textbf{\texttt{print}}(x, k) \mapsto {\bind{(v, s)}{\continue{k}{()}}{\return{(v, \texttt{f"\{$x$\};"} \text{ \^\ } s)}}}\}
  \end{array}
\]
with the type $\handlerType{\effectType[\{ \textbf{\texttt{print}} \}]{\mathbb{N}}}{\effectType[\emptyset]{(\mathbb{N} \times \textsf{String})}}$. We apply the \textsc{Op-Handler} rule, which is transcribed below. Preconditions are numbered for reference. 
\[\inferrule[(Op-Handler)]
    { (1)\,  \texttt{op}: A \to B \in \Sigma \\\\
     (2)\, \type{h}{\handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}}}\\
      (3)\,\type[, x:A, k:{\continuationType[\Delta']{B}{T_2}} ]{c}{\effectType[\Delta']{T_2}}\\
      (4)\,\Delta' \subseteq \Delta \setminus \{ \texttt{op} \} \\
      (5)\, \opHandler{x'}{k'}{c'} \notin h}
    {\type{h ; \opHandler{x}{k}{c}}{\handlerType{\effectType{T_1}}{\effectType[\Delta']{T_2}}}}\]
The preconditions of the \textsc{Op-Handler} rule direct us to check, in turn:
\begin{enumerate}
  \item[(1)] $\textbf{\texttt{print}}: \mathbb{N} \to 1 \in \Sigma$, which is true by assumption
  \item[(2)] Recursively check the rest of the handler $h = \returnHandler{x}{\return{(x, \texttt{""})}}$, ensuring it has type $\handlerType{\effectType[\{ \textbf{\texttt{print}} \}]{\mathbb{N}}}{\effectType[\emptyset]{(\mathbb{N} \times \textsf{String})}}$. This follows from a trivial application of the \textsc{Ret-Handler} rule. 
  \item[(3)] Assuming $x$ has type $\mathbb{N}$ and $k$ has type $\continuationType[\emptyset]{1}{(\mathbb{N} \times \textsf{String})}$, the body \[{\bind{(v, s)}{\continue{k}{()}}{\return{(v, \texttt{f"\{$x$\};"} \text{ \^\ } s)}}}\] has type $\effectType[\emptyset]{(\mathbb{N} \times \textsf{String})}$. This is easy to show.
  \item[(4)] That the handler \textit{only} removes \textbf{\texttt{print}} from the effects row, and no other effects. This check passes, but would fail if we tried to type the handler with, for example, $\handlerType{\effectType[\{ \textbf{\texttt{print}}, \textbf{\texttt{get}} \}]{\mathbb{N}}}{\effectType[\emptyset]{(\mathbb{N} \times \textsf{String})}}$
  \item[(5)] That there are no other handlers for \textbf{\texttt{print}} in $h$.
\end{enumerate}
A full typing derivation may be found in the appendix.

\subsection{The Design Space of Effect Handlers}\label{subsection:effect-handler-design}

\section{Scope Extrusion}\label{section:scope-extrusion-technical}
I now turn my attention to scope extrusion, which arises from the unexpected interaction of effects and metaprogramming. To illustrate scope extrusion, I will first extend \efflang{} (\Cpageref{subsection:effect-handler-calculus}) with AST constructors $\Var{x}{T}$, $\texttt{Nat}(n)$, $\texttt{Lam}$, and $\texttt{Plus}$. For example, we may generate the AST of $\lambda x: \mathbb{N}. \, x + 0$ as follows:

\begin{eff}
$\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{Plus}(\Var{x}{\mathbb{N}}, \texttt{Nat}(0))}}$
\end{eff}

\Cref{listing:efflang-scope-extrusion} illustrates the problem of scope extrusion. The program constructs the AST of $\lambda x: \mathbb{N}. \, x$, but additionally performs an effect, \textbf{\texttt{extrude}}, with type $\mathbb{N} \,  \texttt{expr} \to \mathbb{N} \, \texttt{expr}$. The handler for \textbf{\texttt{extrude}} discards the continuation, simply returning the value it was given: $\Var{x}{\mathbb{N}}$. The entire program evaluates to $\Var{x}{\mathbb{N}}$, and the generated AST is ill-scoped. We say that the result of evaluation demonstrates scope extrusion.

\begin{code}
  \begin{efflst}
    $\begin{array}{l}
      \textbf{\texttt{handle}} \\
      \quad \bind{\texttt{body}}{\textbf{\texttt{extrude}}({\Var{x}{\mathbb{N}}})}{\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{body}}}} \\
      \textbf{\texttt{with}} \\
      \quad \{ \textbf{\texttt{return}}(u) \mapsto {\return{\texttt{Nat}(0)}}; \\
      \quad \; \, \textbf{\texttt{extrude}}(y, k) \mapsto {\return{y}}\}\\
    \end{array}$

    \vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{\Var{x}{\mathbb{N}}}\end{array}$}

  \end{efflst}
  \captionof{listing}{A \efflang{} program that evaluates to the $\Var{x}{\mathbb{N}}$. The AST is ill-scoped, and thus exhibits scope extrusion. It will be used as a running example.}
  \label{listing:efflang-scope-extrusion}
  \end{code}

It is difficult to give a precise definition to scope extrusion, because there are multiple competing definitions \citep{kiselyov-14,kiselyov-16}, and many are given informally. For example, is scope extrusion a property of the \textit{result} of evaluation \citep{kiselyov-16}, as in \Cref{listing:efflang-scope-extrusion}, or is it a property of \textit{intermediate} configurations \citep{kiselyov-14}? We can, for example, build ASTs with extruded variables, that are bound at some future point. In \Cref{listing:efflang-maybe-scope-extrusion}, we produce the intermediate AST $\texttt{Plus}(\texttt{Nat}({0}), \Var{x}{\mathbb{N}})$, which is not well scoped. However, the result of evaluation is well scoped: $\Lam{\Var{x}{\mathbb{N}}}{\texttt{Plus}(\texttt{Nat}({0}), \Var{x}{\mathbb{N}})}$. Does \Cref{listing:efflang-maybe-scope-extrusion} exhibit scope extrusion?

Nevertheless, all definitions agree on the example in \Cref{listing:efflang-scope-extrusion}. Making precise the competing definitions of scope extrusion these competing definitions, and their relation to one another, is a contribution of this dissertation. 
%  This makes scope extrusion difficult to define precisely, and to detect statically. 
% 
% While the example in \Cref{listing:efflang-scope-extrusion} clearly displays scope extrusion, not every example is quite so straightforward. 

% For example, consider the example in \Cref{listing:efflang-no-scope-extrusion}. The program evaluates to the well-scoped AST $\Lam{\Var{x}{\mathbb{N}}}{\Var{x}{\mathbb{N}}}$. It is thus reasonable to claim that the program does \textbf{not} display scope extrusion. However, proving this requires knowledge of the \textit{semantics} of \textbf{\texttt{extrude}}. 

\begin{code}
  \begin{efflst}
    $\begin{array}{l}
      \textbf{\texttt{handle}} \\
      \quad \bind{\texttt{body}}{\textbf{\texttt{extrude}}({\Var{x}{\mathbb{N}}}); \return{\Var{x}{\mathbb{N}}}}{\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{body}}}} \\
      \textbf{\texttt{with}} \\
      \quad \{ \textbf{\texttt{return}}(u) \mapsto \return{u}; \\
      \quad \; \, \textbf{\texttt{extrude}}(y, k) \mapsto { \bind{z}{\return{\texttt{Plus}({\texttt{Nat}({0})}, y)}}{\continue{k}{z}}} \}\\
    \end{array}$

    \vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{Plus}(\texttt{Nat}({0}), \Var{x}{\mathbb{N}})}}\end{array}$}

\end{efflst}
\captionof{listing}{A \efflang{} program that may, or may not demonstrate scope extrusion, depending on one's definition. The final result of the program is well-scoped, but not all intermediate results are well-scoped. If scope extrusion is a property of the resulting AST, then this does not display scope extrusion. If, instead, it is a property of intermediate ASTs, then this does display scope extrusion.}
\label{listing:efflang-maybe-scope-extrusion}
\end{code}

% \Cref{listing:efflang-scope-extrusion} should also hint at why a naÃ¯ve type system fails to prevent scope extrusion. Scope extrusion is a property of \textit{data}

\subsection{Existing Solutions to the Scope Extrusion Problem}
There are multiple solutions to the problem of scope extrusion. The solution space can be broadly divided into two types of approaches: static and dynamic. I will now survey two dynamic approaches, which I term the lazy and eager checks, and one static approach, the method of refined environment classifiers \citep{kiselyov-16,isoda-24}, and. 

\subsubsection{Lazy Dynamic Check}
Scope extrusion at least, of the kind in \Cref{listing:efflang-no-scope-extrusion}, may seem trivial to resolve: evaluate the program to completion, and check that the resulting AST is well-scoped \citep{kiselyov-14}. I term this the Lazy Dynamic Check. This approach, while clearly correct and maximally expressive, is not ideal for efficiency and error reporting reasons. 

To illustrate the inefficiency of this approach, consider a slight variation of \Cref{listing:efflang-scope-extrusion}, \Cref{listing:efflang-lazy-scope-extrusion-inefficient}. In \Cref{listing:efflang-lazy-scope-extrusion-inefficient}, we can, in theory, report a warning as soon scope extrusion is detected. However, waiting for the result of the program can be much more inefficient. 

\begin{code}
  \begin{efflst}
    $\begin{array}{l}
      \textbf{\texttt{do}} \,  x \leftarrow \textbf{\texttt{handle}} \\
      \quad \quad \quad \quad \,\, \bind{\texttt{body}}{\textbf{\texttt{extrude}}({\Var{x}{\mathbb{N}}})}{\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{body}}}} \\
      \quad \quad \quad \,\, \textbf{\texttt{with}} \\
      \quad \quad \quad \quad \, \, \{ \textbf{\texttt{return}}(u) \mapsto {\return{\texttt{Nat}(0)}}; \\
      \quad \quad \quad \quad \, \, \; \, \textbf{\texttt{extrude}}(y, k) \mapsto {\return{y}}\}\\
      \textbf{\texttt{in}} \text{ some very long program}; \return{x}
    \end{array}$

    \vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{\Var{x}{\mathbb{N}}}\end{array}$}

  \end{efflst}
  \captionof{listing}{A \efflang{} program that evaluates to the $\Var{x}{\mathbb{N}}$. Executing the entire program to determine if it exhibits scope extrusion is inefficient.}
  \label{listing:efflang-lazy-scope-extrusion-inefficient}
  \end{code}

In terms of error reporting, note that, in waiting for the result of execution, we lose information about \textit{which program fragment} was responsible for scope extrusion, reducing the informativeness of reported errors \citep{kiselyov-14}.  

\subsubsection{Eager Dynamic Check}
A second dynamic check, motivated by the problems with the Lazy Dynamic Check, adopts a stricter definition of scope extrusion. During the code generation process, one inserts checks into the running program, reporting errors when one encounters  unbound free variables in intermediate ASTs. Hence, the Eager Dynamic Check would classify the program in \Cref{listing:efflang-maybe-scope-extrusion} as exhibiting scope extrusion. The Eager Dynamic Check has been adopted by BER MetaOCaml, and offers better efficiency and error reporting guarantees over the Lazy Dynamic Check \citep{kiselyov-14}.

However, the Eager Dynamic Check is not without issue. The problem relates to the 
manner in which checks are inserted, which we will make precise later. To illustrate the problem, consider \Cref{listing:efflang-maybe-not-scope-extrusion}, a slight variation of \Cref{listing:efflang-maybe-scope-extrusion} in which we replace the program fragment $\texttt{Plus}({\texttt{Nat}({0})}, y)$ with $y$. 

\begin{code}
  \begin{efflst}
    $\begin{array}{l}
      \textbf{\texttt{handle}} \\
      \quad \bind{\texttt{body}}{\textbf{\texttt{extrude}}({\Var{x}{\mathbb{N}}}); \return{\Var{x}{\mathbb{N}}}}{\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{body}}}} \\
      \textbf{\texttt{with}} \\
      \quad \{ \textbf{\texttt{return}}(u) \mapsto \return{u}; \\
      \quad \; \, \textbf{\texttt{extrude}}(y, k) \mapsto { \bind{z}{\return{y}}{\continue{k}{z}}} \}\\
    \end{array}$

    \vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{Plus}(\texttt{Nat}({0}), \Var{x}{\mathbb{N}})}}\end{array}$}

\end{efflst}
\captionof{listing}{A \efflang{} program that is a slight variation of \Cref{listing:efflang-maybe-scope-extrusion}, but that (unlike \Cref{listing:efflang-maybe-scope-extrusion}) passes the Eager Dynamic Check.}
\label{listing:efflang-maybe-not-scope-extrusion}
\end{code}
While the program in \Cref{listing:efflang-maybe-not-scope-extrusion} produces an intermediate AST with extruded variables, because of the mechanism for inserting checks, it passes the Eager Dynamic Check. I assert that this behaviour is unintuitive, and exposes too much of the internals to the programmer. 

\subsubsection{Refined Environment Classifiers} 
Refined Environment Classifiers are a static check that uses the type system to prevent scope extrusion. Recall that metaprogramming involves the \textit{creation} and \textit{manipulation} of ASTs. Refined environment classifiers prevent scope extrusion by checking:

\begin{enumerate}
  \item \textit{Created} ASTs are well-scoped
  \item \textit{Manipulating} ASTs preserves well-scopedness 
\end{enumerate}

We shall first ignore the ability to manipulate ASTs, and consider how to ensure \textit{created} ASTs are well-scoped. What does it mean to be well-scoped? Consider \Cref{fig:refined-enviroment-classifiers-basic}, the AST of 
\[(\lambda f. \lambda x. f x) (\lambda y. y)\]
Informally, a scope represents a set of variables that are permitted to be free. In the example, there are four scopes: one where no variables are free, one where only $f$ is free, one where $f$ and $x$ are free, and one where $y$ is free. An AST is well-scoped \textit{at a scope} if it is well-typed, and where the only free variables are those permitted by the scope. 

Refined environment classifiers make this notion precise. Each classifier represents a scope. The AST has four scopes, corresponding to four classifiers:
\begin{itemize}
\item[$\gamma_{\bot}$\,\,] The top-level, where no free variables are permitted 
\item[$\gamma_{f}$\;\,] Only $\texttt{Var}(f)$ permitted to be free
\item[$\gamma_{fx}$] Only $\texttt{Var}(f)$ and $\texttt{Var}(x)$ permitted to be free 
\item[$\gamma_{y}$\;\,] Only $\texttt{Var}(y)$ permitted to be free
\end{itemize}
As every variable binder creates a scope, we may refer to the classifier created by $\texttt{Var}(\alpha)$, $\textsf{classifier}(\texttt{Var}(\alpha))$. For example, $\gamma_{fx}= \textsf{classifier}(\texttt{Var}(x))$.

\begin{figure}
\centering
\begin{tikzpicture}[level 1/.style={sibling distance=20em},
                    level 2/.style={sibling distance=8em},
                    level 3/.style={sibling distance=7em},
                    level 4/.style={sibling distance=4em}]

  \begin{scope}[every node/.style={font=\ttfamily}]
  \node (app) {App}
  child {node (lam-1) {Lam}
    child {node (var-1) {Var($f$)}}
    child {node (lam-2) {Lam}
           child {node (var-2) {Var($x$)}}
           child {node (app-2) {App}
                  child {node (var-1-use) {Var($f$)} }
                  child {node (var-2-use) {Var($x$)} }
           }
          }
  }
  child {node (lam-3) {Lam}
  child {node (var-3) {Var($y$)}}
  child {node (var-3-use) {Var($y$)}}};
  \end{scope}



  \begin{scope}
    \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(var-3-use.south east) + (0.4cm,-3.7cm)$) rectangle ($(var-1.north west) + (-0.3cm,3.3cm)$);
    
    \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(var-2-use.south east) + (0.35cm,-0.4cm)$) rectangle ($(lam-2.north west) + (-2.0cm,0.2cm)$);
    \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(var-2-use.south east) + (0.05cm,-0.1cm)$) rectangle ($(app-2.north west) + (-1.3cm,0.1cm)$);
    \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(var-3-use.south east) + (0.2cm,-0.2cm)$) rectangle ($(var-3-use.north west) + (-0.2cm,0.2cm)$);

    % \fill[rounded corners, envclassifier1] ($(var-1.north west) + (-0.3cm,3cm)$) rectangle ($(var-1.north west) + (-0.3cm,3cm)$);
  \end{scope}

  \begin{scope}[every node/.style={rounded corners, fill=envclassifier2, font=\scriptsize}]
    \node (gammabot) at ($(var-3.north east) + (2cm,3.3cm)$) {$\gamma_{\bot}$};
    \node (gammaf) at ($(lam-2.north west) + (2.7cm,0.2cm)$) {$\gamma_{f}$};
    \node (gammafx) at ($(app-2.north west) + (1.3cm,0.1cm)$) {$\gamma_{fx}$};
    \node (gammafx) at ($(var-3-use.north west) + (1.2cm,0.2cm)$) {$\gamma_{fx}$};
  \end{scope}
  
\end{tikzpicture}
\caption{The AST of $(\lambda f. \lambda x. f x) (\lambda y.y)$, where each scope is labelled with the corresponding environment classifier.}
\label{fig:refined-enviroment-classifiers-basic}
\end{figure}

With classifiers, we may be precise about ``the variables permitted to be free (within the scope)''. As illustrated by the nesting in \Cref{fig:refined-enviroment-classifiers-basic}, scopes are related to other scopes. For example, since the scope $\gamma_{fx}$ is created within scope $\gamma_{f}$, any variable tagged with $\gamma_f$ may be safely used in the scope $\gamma_{fx}$. We say $\gamma_f$ is compatible with $\gamma_{fx}$, and write 
\[\gamma_{f} \sqsubseteq \gamma_{fx}\]
The compatibility relation ($\sqsubseteq$) is a partial order. It is reflexive, expressing that  $\texttt{Var}(\alpha)$ may be used within the scope it creates
\[\forall \gamma. \gamma \sqsubseteq \gamma \] 
anti-symmetric, since nesting only proceeds in one direction
\[\forall \gamma_1, \gamma_2. \gamma_1 \sqsubseteq \gamma_2 \land \gamma_2 \sqsubseteq \gamma_1 \implies \gamma_1 = \gamma_2 \] 
and transitive 
\[\forall \gamma_1, \gamma_2, \gamma_3. \gamma_1 \sqsubseteq \gamma_2 \land \gamma_2 \sqsubseteq \gamma_3 \implies \gamma_1 \sqsubseteq \gamma_3\]
$\gamma_{\bot}$ acts as the least element of this partial order
\[\forall \gamma, \gamma_{\bot} \sqsubseteq \gamma \] 
In other words, $(\{ \gamma \}, \sqsubseteq)$ form a domain. 

Given a classifier $\gamma$, we may now define the variables permitted to be free in $\gamma$, written $\textsf{permitted}(\gamma)$
\[\textsf{permitted}(\gamma) \triangleq \{ \texttt{Var}(\alpha) \mid \textsf{classifier}(\texttt{Var}(\alpha)) \sqsubseteq \gamma \}\]
For example, $\textsf{permitted}(\gamma_{fx}) = \{ \texttt{Var}(f), \texttt{Var}(x)\}$ 

We now say that an AST $n$ is well-scoped at type $T$ and scope $\gamma$ if it is well-typed at $T$, and all free variables in $n$ are in $\textsf{permitted}(\gamma)$. We write 
\[\Gamma \vdash^{\gamma} n : T \]
% \[ \textsf{permitted}(\texttt{Var}(\alpha)) \triangleq \{ \gamma \mid \textsf{classifier}(\texttt{Var}(\alpha)) \sqsubseteq \gamma \} \]

Ensuring that created ASTs are well-scoped is the responsibility of the type system. The key rule is the \textsc{C-Abs} rule, which, \textbf{assuming we know that we are creating an AST}, has roughly the following shape \footnote{I will revisit this assumption when introducing the type system for my calculus}: 
\[\inferrule[(C-Abs)]{\, \gamma \in \Gamma \\ (2) \, \gamma' \text{fresh} \\ (3) \, \Gamma, \gamma', \gamma \sqsubseteq \gamma', (x: T_1)^{\gamma'} \vdash^{\gamma'} n: T_2}{(1) \, \Gamma \vdash^{\gamma} \lambda x. n: T_1 \longrightarrow T_2}\]
For reference, the premises and conclusions have been numbered, and many  technical details have been simplified for clarity. \Cref{fig:cref-typing-rule} visually depicts of the typing rule. 

\begin{enumerate}
  \item[(1)] The goal of the typing rule is to ensure the function is well-scoped at type $T_1 \to T_2$ and scope $\gamma$.
  \item[(2)] Since the function introduces a new variable binder, $\texttt{Var}(x)$, one has to create a new scope. This is achieved by picking a fresh classifier $\gamma'$.
  \item[(3)] We record the following:
  \begin{enumerate}
    \item[(a)] Since $\gamma'$ is created within the scope of $\gamma$, $\gamma \sqsubseteq \gamma'$.
    \item[(b)] $\textsf{classifier}(\texttt{Var}(x)) = \gamma'$ (as a shorthand $(x: T_1)^{\gamma'}$)
  \end{enumerate}
  With this added knowledge, we ensure that the function body is well-scoped at type $T_2$ and $\gamma'$.
\end{enumerate}
\begin{figure}
  \centering
  \begin{tikzpicture}[level 1/.style={sibling distance=10em}]
  
    \begin{scope}[every node/.style={font=\ttfamily}]
    \node (lam) {Lam}
    child {node (binder) {Var$(x)$}}
    child {node (body) {n}};
    \end{scope}
  
  
  
    \begin{scope}
      \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(body.south east) + (1.2cm,-.5cm)$) rectangle ($(binder.north west) + (-0.9cm,1.8cm)$);
      
      % \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(var-2-use.south east) + (0.35cm,-0.4cm)$) rectangle ($(lam-2.north west) + (-2.0cm,0.2cm)$);
      % \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(var-2-use.south east) + (0.05cm,-0.1cm)$) rectangle ($(app-2.north west) + (-1.3cm,0.1cm)$);
      \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(body.south east) + (-1cm,-0.2cm)$) rectangle ($(body.north west) + (1cm,0.2cm)$);
  
      % \fill[rounded corners, envclassifier1] ($(var-1.north west) + (-0.3cm,3cm)$) rectangle ($(var-1.north west) + (-0.3cm,3cm)$);
    \end{scope}
  
    \begin{scope}[every node/.style={rounded corners, fill=envclassifier2, font=\scriptsize}]
      \node (gammabot) at ($(binder.north west) + (5.3cm,1.8cm)$) {$\gamma$};
      \node (gammaf) at ($(body.north west) + (.6cm,0.2cm)$) {$\gamma'$};
    \end{scope}
    
  \end{tikzpicture}
  \caption{Visual depiction of the \textsc{(C-Abs)} typing rule.}
  \label{fig:cref-typing-rule}
  \end{figure}
  
% Having defined a notion of compatibility, we may now 

The above example focused on \textbf{creating} ASTs, and had no compile-time executable code. We now consider how to maintain well-scopedness while \textbf{manipulating} ASTs. We consider the ``AST'' of the scope extrusion example, \Cref{listing:efflang-scope-extrusion} (\Cref{fig:classifier-ast-scope-extrusion}). Notice that in place of AST nodes, we may now have compile-time executable code that \textit{evaluate} to AST nodes. Thus, both code and AST nodes reside within scopes. We have two classifiers: $\gamma_{\bot}$ and $\gamma_{\alpha}$, with $\textsf{classifier}(\texttt{Var}(\alpha)) = \gamma_{\alpha}$.

\begin{figure}
\centering

\begin{tikzpicture}[level 1/.style={sibling distance=15em}]

  \begin{scope}[every node/.style={font=\ttfamily}]
  \node (handler) {Lam}
  child {node (var) {Var($\alpha$)}}
  child {node (body) {\textbf{extrude}(\texttt{Var}($\alpha$)); $\return(\texttt{Var}(\alpha))$}};
  \end{scope}

  \begin{scope}
    \node[anchor=east] (lam-left) at ($(handler.west) + (-0.1cm, 0cm)$) {\textbf{\texttt{handle}}};
    \node[anchor=west] (lam-right) at ($(handler.east) + (0.1cm, 0cm)$) {\texttt{\textbf{with} \{$\ldots$,\textbf{extrude}($y, k$)$\mapsto\ldots$\} }};
  \end{scope}


  \begin{scope}[on background layer]
    \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(body.south east) + (0.5cm,-0.4cm)$) rectangle ($(lam-left.north west) + (-2.1cm,0.4cm)$);

    \draw[line width=0.5mm, rounded corners,envclassifier1, dashed] ($(body.south east) + (0.1cm,-0.1cm)$) rectangle ($(body.north west) + (-0.2cm,0.2cm)$);

    \begin{scope}[every node/.style={rounded corners, fill=envclassifier2, font=\scriptsize}]
      \node (gammabottom) at ($(lam-left.north west) + (7.6cm,0.4cm)$) {$\gamma_{\bot}$};
      \node (gammaalpha) at ($(body.north west) + (5.8cm,0.2cm)$) {$\gamma_{\alpha}$};
    \end{scope}
    % \draw[line width=0.5mm, rounded corners,envclassifier2a] ($(var-2-use.south east) + (0.05cm,-0.1cm)$) rectangle ($(app-2.north west) + (-1.3cm,0.1cm)$);
    % \draw[line width=0.5mm, rounded corners,envclassifier1] ($(var-3-use.south east) + (0.1cm,-0.1cm)$) rectangle ($(var-3-use.north west) + (-0.1cm,0.1cm)$);

    % \fill[rounded corners, envclassifier1] ($(var-1.north west) + (-0.3cm,3cm)$) rectangle ($(var-1.north west) + (-0.3cm,3cm)$);
  \end{scope}
  
\end{tikzpicture}

\caption{The ``AST'' of the scope extrusion example, \Cref{listing:efflang-scope-extrusion}. Notice that in place of AST nodes, we may now have compile-time executable code that \textit{evaluate} to AST nodes.}
\label{fig:classifier-ast-scope-extrusion}
\end{figure}

Key to the prevention of scope extrusion is the typing of handlers and operations, like \textbf{\texttt{extrude}}, that manipulate ASTs. Since these rules are complex, we describe them informally. The handle expression
\[\handleWith{e}{h}\]
is in scope $\gamma_{\bot}$. Therefore, for each operation handled by $h$, such as \textbf{\texttt{extrude}}, the argument to the operation must either not be an AST, or be an AST that is well-scoped at some $\gamma \sqsubseteq \gamma_{\bot}$. 
% We 
% \[\textbf{\texttt{extrude}}: \texttt{AST}(\mathbb{N})^{\gamma_{\bot}} \to 1^{\gamma_{\bot}}\]
However, $\texttt{Var}(\alpha)$ is typed at $\gamma_{\alpha}$, and clearly, $\gamma_{\alpha} \not\sqsubseteq \gamma_{\bot}$. There is thus no way to type the scope extrusion example in \Cref{listing:efflang-scope-extrusion}.

Note that the analysis was independent of the \textit{body} of the handler. Therefore, the examples in \Cref{listing:efflang-maybe-not-scope-extrusion,listing:efflang-maybe-scope-extrusion} are \textit{also} not well-typed. Perhaps somewhat surprisingly, so too is \Cref{listing:efflang-no-scope-extrusion} (which would pass both the Eager and Lazy Dynamic Checks). Refined environment classifiers statically prevent variables ($\texttt{Var}(\alpha)$) from becoming \textit{available} in program fragments ($\texttt{\textbf{op}}(y, k) \mapsto \ldots$) where, \textit{if misused}, \textit{might} result in scope extrusion. This is, of course, an over-approximation. It means that the refined environment classifiers check prevents not only both types of scope extrusion, but even more benign examples, such as that in \Cref{listing:efflang-no-scope-extrusion}.

\begin{code}
  \begin{efflst}
    $\begin{array}{l}
      \textbf{\texttt{handle}} \\
      \quad \bind{\texttt{body}}{\textbf{\texttt{extrude}}({\Var{x}{\mathbb{N}}}); \return{\Var{x}{\mathbb{N}}}}{\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{body}}}} \\
      \textbf{\texttt{with}} \\
      \quad \{ \textbf{\texttt{return}}(u) \mapsto \return{u}; \\
      \quad \; \, \textbf{\texttt{extrude}}(y, k) \mapsto { \return{\texttt{Nat}(0)}} \}\\
    \end{array}$

    \vspace{2mm} 
\textcolor{effComment}{\hrule height 0.2mm \relax}
\vspace{2mm} 

\textcolor{effComment}{$\begin{array}{l}\return{\Lam{\Var{x}{\mathbb{N}}}{\texttt{Plus}(\texttt{Nat}({0}), \Var{x}{\mathbb{N}})}}\end{array}$}

\end{efflst}
\captionof{listing}{A \efflang{} program that passes the Eager and Lazy Dynamic Checks, but is not well-typed under the Refined Environment Classifiers type system.}
\label{listing:efflang-no-scope-extrusion}
\end{code}

\Cref{listing:efflang-no-scope-extrusion} thus illustrates one of the key drawbacks of Refined Environment Classifiers: the check is too stringent, and restricts expressiveness. 
% Scope extrusion is difficult to define precisely, because there are multiple reasonable definitions. Consider the example in \Cref{listing:efflang-maybe-scope-extrusion}, which returns the well-scoped AST $\Lam{\Var{x}{\mathbb{N}}}{\texttt{Plus}(\texttt{Nat}({0}), \Var{x}{\mathbb{N}})}$. If scope extrusion is a property of the \textit{final} AST, then the program does not display scope extrusion. This is the definition implied by \citet{kiselyov-16}. However, in the process of computation, the program will evaluate to the intermediate AST $\texttt{Plus}(\texttt{Nat}({0}), \Var{x}{\mathbb{N}})$, which is not well scoped. It is only via use of the continuation $k$ that the error is resolved. If scope extrusion is a property of \textit{intermediate results} \citep{kiselyov-14}, then \Cref{listing:efflang-maybe-scope-extrusion} \textit{does} result in scope extrusion.  
% As a result, most solutions seek to detect scope extrusion more eagerly. These solutions can be significantly more efficient, and have better error reporting capabilities. To be more eager, we may need to detect approximations of scope extrusion, and thus report false positives. These solutions thus sacrifice expressiveness. The solution space may be divided broadly into two categories: static (type-based) solutions, and dynamic solutions. 

% We have already seen one dynamic solution, the Lazy check. I will now briefly introduce one static solution, Refined Environment Classifiers, and one more dynamic solution, the Eager check. For each, I will illustrate how the solution safely approximates scope extrusion. 

% Regardless which definition we choose, scope extrusion is difficult to prevent using a type system. Critically, scope extrusion is closely related to the dynamic semantics of the program, rather than the syntactic structure. Consider the program below, which 

% Critically, it is difficult  example in \Cref{listing:efflang-scope-extrusion} additionally illustrates the subtlety 